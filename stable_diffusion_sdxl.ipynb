{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e9350cb4e9a4439b0cc9fc8316b2a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90c41d5116f147ee95ce7421b7039a7e",
              "IPY_MODEL_8b7680a8d0c24accaa7bc56674bf68f4",
              "IPY_MODEL_5d9bfc736507464fa223cdb1576d2e1e"
            ],
            "layout": "IPY_MODEL_7748ef307fc140edb8cd794327119219"
          }
        },
        "90c41d5116f147ee95ce7421b7039a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1df0054e9fbc4e96bd807ca1e9bd2b5d",
            "placeholder": "​",
            "style": "IPY_MODEL_d8a2524d3f3d499f9aaadd882ee67b1d",
            "value": "Steps:   0%"
          }
        },
        "8b7680a8d0c24accaa7bc56674bf68f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8b1027297304b738deb83c8d07d13e1",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb3c99057f9c4a64a7056d11adafbcc4",
            "value": 0
          }
        },
        "5d9bfc736507464fa223cdb1576d2e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_071901cf9ab54eefbbde302dbfadb2df",
            "placeholder": "​",
            "style": "IPY_MODEL_e06b622e5a814b0eb81f105818f18283",
            "value": " 0/612 [00:00&lt;?, ?it/s]"
          }
        },
        "7748ef307fc140edb8cd794327119219": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1df0054e9fbc4e96bd807ca1e9bd2b5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8a2524d3f3d499f9aaadd882ee67b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8b1027297304b738deb83c8d07d13e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb3c99057f9c4a64a7056d11adafbcc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "071901cf9ab54eefbbde302dbfadb2df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e06b622e5a814b0eb81f105818f18283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers accelerate torchvision transformers datasets ftfy tensorboard Jinja2 peft wandb bitsandbytes clip pillow==10.3.0"
      ],
      "metadata": {
        "id": "LW_ZlO_ySuQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "880b9fd5-469b-48d9-c7d5-06a9ef92d748"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.12/dist-packages (3.1.6)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: clip in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: pillow==10.3.0 in /usr/local/lib/python3.12/dist-packages (10.3.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.36.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.45.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->diffusers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.11.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "slwez4YvQAZP"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "import clip\n",
        "from pathlib import Path\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "import transformers\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import DistributedDataParallelKwargs, DistributedType, set_seed\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from peft.utils import get_peft_model_state_dict\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "from PIL import Image\n",
        "\n",
        "import diffusers\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    DDPMScheduler,\n",
        "    DiffusionPipeline,\n",
        "    StableDiffusionPipeline,\n",
        "    UNet2DConditionModel,\n",
        ")\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.utils import (\n",
        "    check_min_version,\n",
        "    convert_state_dict_to_diffusers,\n",
        ")\n",
        "from diffusers.utils.import_utils import is_wandb_available\n",
        "from diffusers.utils.torch_utils import is_compiled_module\n",
        "from transformers import CLIPTextModel, CLIPProcessor, CLIPModel, CLIPTextModelWithProjection # Added CLIPTextModelWithProjection\n",
        "from torch.utils.tensorboard import SummaryWriter # Added for SummaryWriter\n",
        "\n",
        "if is_wandb_available():\n",
        "    import wandb\n",
        "\n",
        "check_min_version(\"0.30.0.dev0\")\n",
        "\n",
        "logger = get_logger(__name__, log_level=\"INFO\")\n",
        "\n",
        "\n",
        "def save_lora_state(save_dir: str, unet, unwrap_model_fn):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    unwrapped = unwrap_model_fn(unet)\n",
        "    lora_state = get_peft_model_state_dict(unwrapped)\n",
        "    save_path = os.path.join(save_dir, \"lora_state.pt\")\n",
        "    torch.save(lora_state, save_path)\n",
        "    return save_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_accumulation_steps = 4\n",
        "train_batch_size = 1\n",
        "num_train_epochs = 2\n",
        "max_grad_norm = 4\n",
        "checkpointing_steps = 2\n",
        "checkpoints_total_limit = 5\n",
        "logging_steps = 50\n",
        "\n",
        "# Model parameters\n",
        "pretrained_model_name_or_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "revision = None\n",
        "variant = None\n",
        "LOCAL_MODEL_PATH = \"\" # Base path for local models, e.g., \"/content/models/\"\n",
        "\n",
        "# LoRA loading/saving parameters\n",
        "# If loading an existing LoRA model, specify the path to its directory.\n",
        "# If starting training from scratch, leave empty.\n",
        "source_model_path = \"\" # e.g., \"my_finetuned_lora_weights_dir\" or full path \"/content/my_finetuned_lora_weights_dir\"\n",
        "\n",
        "# Dataset parameters\n",
        "dataset_type = \"huggingface\" # Options: \"huggingface\" or \"imagefolder\"\n",
        "dataset_name = \"lambdalabs/naruto-blip-captions\" # Required for dataset_type=\"huggingface\"\n",
        "dataset_config_name = None # Optional for dataset_type=\"huggingface\"\n",
        "dataset_path = \"\" # Required for dataset_type=\"imagefolder\" and if local data is used\n",
        "image_column = \"image\"\n",
        "caption_column = \"text\"\n",
        "\n",
        "# Training parameters\n",
        "resolution = 512 # Image resolution for training\n",
        "center_crop = False # Whether to center crop the images\n",
        "random_flip = False # Whether to randomly flip images horizontally\n",
        "max_train_samples = None # Number of training samples to use (None for all)\n",
        "dataloader_num_workers = 0 # Number of workers for data loading\n",
        "lr_warmup_steps = 5 # Number of warmup steps for learning rate scheduler\n",
        "max_train_steps = None # Maximum number of training steps (None for derived from num_train_epochs)\n",
        "lr_scheduler = \"constant\" # Learning rate scheduler type\n",
        "use_8bit_adam = True # Whether to use 8-bit Adam optimizer\n",
        "adam_beta1 = 0.9\n",
        "adam_beta2 = 0.999\n",
        "adam_weight_decay = 1e-2\n",
        "adam_epsilon = 1e-04\n",
        "gradient_checkpointing = False # Whether to use gradient checkpointing\n",
        "\n",
        "# Resume and output parameters\n",
        "resume_from_checkpoint = None # \"latest\" or a path to a specific checkpoint, or None\n",
        "output_dir = \"sd-finetune-model\" # Directory to save checkpoints and final model\n",
        "logging_dir = \"logs\" # Directory for TensorBoard logs\n",
        "\n",
        "# Validation and logging\n",
        "wandb_project = \"sd-finetune\" # Set to None to disable Weights & Biases logging\n",
        "num_validation_images = 4 # Number of images to generate for validation"
      ],
      "metadata": {
        "id": "745eM9ve18eU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StableDiffusionTrainer:\n",
        "    def __init__(self, output_dir, num_train_epochs, max_train_steps, logging_steps, train_dataloader, vae, unet, text_encoder, text_encoder_2, noise_scheduler, optimizer, lr_scheduler, accelerator, weight_dtype, tensorboard_writer, logger, progress_bar, first_epoch, initial_global_step, unwrap_model):\n",
        "        self.output_dir = output_dir\n",
        "        self.num_train_epochs = num_train_epochs\n",
        "        self.max_train_steps = max_train_steps\n",
        "        self.logging_steps = logging_steps\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.vae = vae\n",
        "        self.unet = unet\n",
        "        self.text_encoder = text_encoder\n",
        "        self.text_encoder_2 = text_encoder_2 # Correctly initialize text_encoder_2\n",
        "        self.noise_scheduler = noise_scheduler\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.accelerator = accelerator\n",
        "        self.weight_dtype = weight_dtype\n",
        "        self.tensorboard_writer = tensorboard_writer\n",
        "        self.logger = logger\n",
        "        self.global_step = initial_global_step\n",
        "        self.progress_bar = progress_bar\n",
        "        self.first_epoch = first_epoch\n",
        "        self.metrics = {}\n",
        "        self.unwrap_model = unwrap_model\n",
        "        self.train_batch_size = getattr(self.accelerator.state, \"train_batch_size\", None)\n",
        "        if self.train_batch_size is None:\n",
        "            self.train_batch_size = train_dataloader.batch_size\n",
        "\n",
        "        # If you pass these in args, replace with args.xxx\n",
        "        self.gradient_accumulation_steps = accelerator.gradient_accumulation_steps\n",
        "        self.max_grad_norm = getattr(self, \"max_grad_norm\", 1.0)\n",
        "\n",
        "        # Optional configs — fill with safe defaults if not provided\n",
        "        self.checkpointing_steps = 500\n",
        "        self.checkpoints_total_limit = 3\n",
        "        self.wandb_project = None\n",
        "\n",
        "    # Added for SDXL\n",
        "    def compute_time_ids(self, original_size, crops_coords_top_left, resolution):\n",
        "        bs = original_size.shape[0]\n",
        "        res_tensor = torch.tensor([resolution, resolution], device=self.accelerator.device, dtype=original_size.dtype)\n",
        "        orig_and_crop = torch.cat([original_size.to(self.accelerator.device), crops_coords_top_left.to(self.accelerator.device)], dim=1)\n",
        "        add_time_ids = torch.cat([orig_and_crop, res_tensor.unsqueeze(0).repeat(bs, 1)], dim=1)\n",
        "        return add_time_ids.to(self.accelerator.device, dtype=self.weight_dtype)\n",
        "\n",
        "    def train(self):\n",
        "        train_batch_size = self.train_batch_size\n",
        "        grad_acc_steps = self.gradient_accumulation_steps\n",
        "        for epoch in range(self.first_epoch, self.num_train_epochs):\n",
        "            self.unet.train()\n",
        "            train_loss = 0.0\n",
        "            current_loss = 0.0\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "                with self.accelerator.accumulate(self.unet):\n",
        "                    device = self.accelerator.device\n",
        "                    self.vae.to(device, dtype=torch.float32)\n",
        "                    pixel_values = batch[\"pixel_values\"].to(device=device, dtype=torch.float32, non_blocking=True)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        # use autocast for the frozen modules so they use fp16/bf16 if accelerator configured\n",
        "                        with self.accelerator.autocast():\n",
        "                            # pixel_values = batch[\"pixel_values\"].to(device=self.accelerator.device, dtype=self.weight_dtype)\n",
        "                            vae_out = self.vae.encode(pixel_values)\n",
        "                            # latents = vae_out.latent_dist.sample() * self.vae.config.scaling_factor\n",
        "                            latents = vae_out.latent_dist.sample()\n",
        "                            # convert latents to training weight dtype (fp16/bf16) for UNet forward\n",
        "                            model_input = (latents * self.vae.config.scaling_factor).to(self.weight_dtype).detach()\n",
        "                            self.vae.to(\"cpu\")\n",
        "                            torch.cuda.empty_cache()\n",
        "\n",
        "                            # text encoders (frozen): move input_ids to device\n",
        "                            input_ids = batch[\"input_ids\"].to(self.accelerator.device)\n",
        "                            input_ids_2 = batch[\"input_ids_2\"].to(self.accelerator.device)\n",
        "\n",
        "                            encoder_hidden_states = self.text_encoder(input_ids)[0]\n",
        "                            te2_out = self.text_encoder_2(input_ids_2, return_dict=True)\n",
        "                            pooled_text_embeds = te2_out.text_embeds\n",
        "                            encoder_hidden_states_2 = te2_out.last_hidden_state\n",
        "                            encoder_hidden_states = torch.cat([encoder_hidden_states, encoder_hidden_states_2], dim=-1)\n",
        "\n",
        "                    # Sample noise & timesteps\n",
        "                    noise = torch.randn_like(model_input)\n",
        "                    bsz = model_input.shape[0]\n",
        "                    timesteps = torch.randint(\n",
        "                        0,\n",
        "                        self.noise_scheduler.config.num_train_timesteps,\n",
        "                        (bsz,),\n",
        "                        device=model_input.device,\n",
        "                    ).long()\n",
        "                    noisy_model_input = self.noise_scheduler.add_noise(model_input, noise, timesteps)\n",
        "\n",
        "                    # compute SDXL time ids\n",
        "                    original_sizes = batch[\"original_sizes\"].squeeze(1)\n",
        "                    crops_coords_top_left = batch[\"crops_coords_top_left\"].squeeze(1)\n",
        "                    add_time_ids = self.compute_time_ids(original_sizes, crops_coords_top_left, resolution=resolution)\n",
        "\n",
        "                    added_cond_kwargs = {\n",
        "                        \"text_embeds\": pooled_text_embeds,\n",
        "                        \"time_ids\": add_time_ids,\n",
        "                    }\n",
        "\n",
        "                    # UNet forward in autocast (mixed precision)\n",
        "                    with self.accelerator.autocast():\n",
        "                        model_pred = self.unet(\n",
        "                            noisy_model_input,\n",
        "                            timesteps,\n",
        "                            encoder_hidden_states,\n",
        "                            added_cond_kwargs=added_cond_kwargs,\n",
        "                            return_dict=False,\n",
        "                        )[0]\n",
        "\n",
        "                    # free heavy intermediates immediately\n",
        "                    del vae_out, latents, encoder_hidden_states_2, te2_out, pooled_text_embeds\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                    # Select target and compute MSE loss\n",
        "                    if self.noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                        target = noise\n",
        "                    else:\n",
        "                        target = self.noise_scheduler.get_velocity(model_input, noise, timesteps)\n",
        "\n",
        "                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "                    # gather for logging across processes\n",
        "                    avg_loss = self.accelerator.gather(loss.detach().repeat(train_batch_size)).mean()\n",
        "                    train_loss += avg_loss.item() / grad_acc_steps\n",
        "                    current_loss = train_loss\n",
        "\n",
        "                    # Backpropagate\n",
        "                    self.accelerator.backward(loss)\n",
        "\n",
        "                    if self.accelerator.sync_gradients:\n",
        "                        # clip grads for only trainable parameters (LoRA)\n",
        "                        trainable_params = []\n",
        "                        for group in self.optimizer.param_groups:\n",
        "                            trainable_params.extend(group['params'])\n",
        "                        total_norm = self.accelerator.clip_grad_norm_(trainable_params, max_grad_norm)\n",
        "                        # optimizer step, scheduler step, zero grad\n",
        "                        self.optimizer.step()\n",
        "                        self.lr_scheduler.step()\n",
        "                        self.optimizer.zero_grad()\n",
        "\n",
        "                # Step-end bookkeeping and checkpointing\n",
        "                if self.accelerator.sync_gradients:\n",
        "                    self.at_step_end(loss, total_norm, epoch, current_loss)\n",
        "                else:\n",
        "                    # still update progress and step count on non-syncing steps\n",
        "                    self.progress_bar.update(1)\n",
        "                    self.global_step += 1\n",
        "                    logs = {\"step_loss\": loss.detach().item(), \"lr\": self.lr_scheduler.get_last_lr()[0] if self.lr_scheduler else 0}\n",
        "                    self.progress_bar.set_postfix(**logs)\n",
        "\n",
        "                if self.global_step >= self.max_train_steps:\n",
        "                    break\n",
        "\n",
        "            self.at_epoch_end(epoch, current_loss)\n",
        "            if self.global_step >= self.max_train_steps:\n",
        "                break\n",
        "\n",
        "    def at_step_end(self, loss, total_norm, epoch, current_loss):\n",
        "        self.progress_bar.update(1)\n",
        "        self.global_step += 1\n",
        "\n",
        "        # ensure grad norm is float32 for logging\n",
        "        if isinstance(total_norm, torch.Tensor) and (total_norm.dtype == torch.bfloat16 or total_norm.dtype == torch.float16):\n",
        "            total_norm = total_norm.to(torch.float32)\n",
        "\n",
        "        if self.global_step % self.logging_steps == 0:\n",
        "            self.tensorboard_writer.add_scalar('train/loss', current_loss, self.global_step)\n",
        "            if self.lr_scheduler is not None:\n",
        "                self.tensorboard_writer.add_scalar('train/learning_rate', self.lr_scheduler.get_last_lr()[0], self.global_step)\n",
        "            self.tensorboard_writer.add_scalar('train/epoch', epoch+1, self.global_step)\n",
        "            self.tensorboard_writer.add_scalar('train/grad_norm', float(total_norm), self.global_step)\n",
        "\n",
        "        if os.environ.get('WANDB_API_KEY') and self.wandb_project:\n",
        "            try:\n",
        "                import wandb\n",
        "                wandb.log({\n",
        "                    \"train_loss\": current_loss,\n",
        "                    \"learning_rate\": self.lr_scheduler.get_last_lr()[0] if self.lr_scheduler else 0,\n",
        "                    \"epoch\": epoch+1,\n",
        "                    \"grad_norm\": float(total_norm)\n",
        "                }, step=self.global_step)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Checkpointing (only main process or DeepSpeed)\n",
        "        if self.accelerator.distributed_type == DistributedType.DEEPSPEED or self.accelerator.is_main_process:\n",
        "            if self.global_step % self.checkpointing_steps == 0:\n",
        "                if self.checkpoints_total_limit is not None:\n",
        "                    checkpoints = [d for d in os.listdir(self.output_dir) if d.startswith(\"checkpoint\")]\n",
        "                    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
        "                    if len(checkpoints) >= self.checkpoints_total_limit:\n",
        "                        num_to_remove = len(checkpoints) - self.checkpoints_total_limit + 1\n",
        "                        for removing_checkpoint in checkpoints[:num_to_remove]:\n",
        "                            shutil.rmtree(os.path.join(self.output_dir, removing_checkpoint))\n",
        "\n",
        "                save_dir = os.path.join(self.output_dir, f\"checkpoint-{self.global_step}\")\n",
        "                self.accelerator.save_state(save_dir)\n",
        "                # Save LoRA/PEFT state dict\n",
        "                save_lora_state(save_dir, self.unet, self.unwrap_model)\n",
        "                self.logger.info(f\"Saved checkpoint to {save_dir}\")\n",
        "\n",
        "        logs = {\"step_loss\": loss.detach().item(), \"lr\": self.lr_scheduler.get_last_lr()[0] if self.lr_scheduler else 0}\n",
        "        self.progress_bar.set_postfix(**logs)\n",
        "\n",
        "    def at_epoch_end(self, epoch, current_loss):\n",
        "        self.logger.info(f\"EPOCH: {epoch+1}, TRAIN_LOSS: {current_loss}\")\n",
        "        self.metrics = {\"epochs\": epoch+1, \"train_loss\": current_loss}\n",
        "\n",
        "    def get_final_metrics(self):\n",
        "        return self.metrics\n"
      ],
      "metadata": {
        "id": "xxRq1RKQQlz3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess as sp\n",
        "\n",
        "def load_tokenizers(args):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        args.pretrained_model_name_or_path,\n",
        "        subfolder=\"tokenizer\",\n",
        "        revision=args.revision,\n",
        "        use_fast=False,\n",
        "    )\n",
        "    # For SDXL, load the second tokenizer as well\n",
        "    tokenizer_2 = AutoTokenizer.from_pretrained(\n",
        "        args.pretrained_model_name_or_path,\n",
        "        subfolder=\"tokenizer_2\",\n",
        "        revision=args.revision,\n",
        "        use_fast=False,\n",
        "    )\n",
        "    return tokenizer, tokenizer_2\n",
        "\n",
        "\n",
        "def load_pretrained_model(args):\n",
        "    load_model_path = args.pretrained_model_name_or_path\n",
        "\n",
        "    # Load the first text encoder\n",
        "    text_encoder = CLIPTextModel.from_pretrained(\n",
        "        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n",
        "    )\n",
        "    # Load the second text encoder for SDXL\n",
        "    text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(\n",
        "        args.pretrained_model_name_or_path, subfolder=\"text_encoder_2\", revision=args.revision\n",
        "    )\n",
        "\n",
        "    noise_scheduler = DDPMScheduler.from_pretrained(load_model_path, subfolder=\"scheduler\")\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        args.pretrained_model_name_or_path,\n",
        "        subfolder=\"vae\",\n",
        "        revision=args.revision,\n",
        "        variant=args.variant,\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    unet = UNet2DConditionModel.from_pretrained(\n",
        "        load_model_path,\n",
        "        subfolder=\"unet\",\n",
        "        revision=args.revision,\n",
        "        variant=args.variant,\n",
        "    )\n",
        "    try:\n",
        "        unet.enable_xformers_memory_efficient_attention()\n",
        "        logger.info(\"xformers enabled\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"xformers not available: {e}\")\n",
        "\n",
        "    # Attention slicing (reduces peak mem)\n",
        "    try:\n",
        "        unet.enable_attention_slicing()\n",
        "        logger.info(\"attention slicing enabled\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"attention slicing failed: {e}\")\n",
        "\n",
        "    return noise_scheduler, text_encoder, text_encoder_2, vae, unet\n",
        "\n",
        "def decode_base64(encoded_string):\n",
        "    import base64\n",
        "    decoded_bytes = base64.b64decode(encoded_string)\n",
        "    decoded_text = decoded_bytes.decode('utf-8')\n",
        "    return decoded_text\n",
        "\n",
        "def gpu_memory():\n",
        "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
        "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
        "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
        "    return memory_free_values"
      ],
      "metadata": {
        "id": "fpsBH2pURmK8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        mixed_precision=\"fp16\", # Using fp16, changed from pf16 for consistency. Can be \"bf16\" as well.\n",
        "        log_with=\"wandb\" if wandb_project else None,\n",
        "        kwargs_handlers=[kwargs],\n",
        "    )\n",
        "    validation_prompt = decode_base64(\" \") # For Validation prompt\n",
        "    # gpufree = gpu_memory() # Commented out, `sp` was not imported in its original location, now fixed in fpsBH2pURmK8. Re-enable if needed.\n",
        "\n",
        "    if accelerator.is_local_main_process:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_warning()\n",
        "        diffusers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "        diffusers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # If passed along, set the training seed now.\n",
        "    seed_size = 25\n",
        "    set_seed(seed_size)\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if accelerator.is_main_process:\n",
        "        if output_dir is not None:\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # if args.push_to_hub:\n",
        "        #     repo_id = create_repo(\n",
        "        #         repo_id=args.hub_model_id or Path(args.output_dir).name, exist_ok=True, token=args.hub_token\n",
        "        #     ).repo_id\n",
        "\n",
        "    # Load the tokenizers\n",
        "    accelerator.print(\"Loading the tokenizers, text endcoders, schedulers and model:\")\n",
        "    tokenizer, tokenizer_2 = load_tokenizers(SimpleNamespace(\n",
        "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "        revision=revision\n",
        "    ))\n",
        "\n",
        "    # import correct text encoder classes, scheduler and models\n",
        "    accelerator.print(\"Loading the Stable diffusion Model:\")\n",
        "    noise_scheduler, text_encoder, text_encoder_2, vae, unet = load_pretrained_model(SimpleNamespace(\n",
        "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "        revision=revision,\n",
        "        variant=variant\n",
        "    ))\n",
        "\n",
        "    # Load the clip_model for CLIP score\n",
        "    accelerator.print(\"Loading the CLIP Model for validation:\")\n",
        "    # Fix: The 'clip' module installed via pip does not have a 'load' attribute.\n",
        "    # We will use CLIPModel and CLIPProcessor from the transformers library instead.\n",
        "    # clip_model, preprocess = clip.load(\"ViT-B/32\", device=accelerator.device)\n",
        "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(accelerator.device)\n",
        "    preprocess = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "    accelerator.print(f\"LOADING PRE-TRAINED MODEL FROM {LOCAL_MODEL_PATH}{source_model_path}\")\n",
        "    trained_model_path = f\"{LOCAL_MODEL_PATH}{source_model_path}\"\n",
        "    if trained_model_path:\n",
        "        unet.load_attn_procs(trained_model_path, weight_name=\"pytorch_lora_weights.safetensors\")\n",
        "        unet.enable_lora()\n",
        "    else:\n",
        "        accelerator.print(\"No source model path provided, LoRA will not be loaded initially.\")\n",
        "\n",
        "    # We only train the additional adapter LoRA layers\n",
        "    vae.requires_grad_(False)\n",
        "    text_encoder.requires_grad_(False)\n",
        "    text_encoder_2.requires_grad_(False)\n",
        "    unet.requires_grad_(False)\n",
        "\n",
        "    # For mixed precision training we cast all non-trainable weights (vae, non-lora text_encoder and non-lora unet) to half-precision\n",
        "    # as these weights are only used for inference, keeping weights in full precision is not required.\n",
        "    weight_dtype = torch.float32\n",
        "    if accelerator.mixed_precision == \"fp16\":\n",
        "        weight_dtype = torch.float16\n",
        "    elif accelerator.mixed_precision == \"bf16\":\n",
        "        weight_dtype = torch.bfloat16\n",
        "\n",
        "    # Freeze the unet parameters before adding adapters\n",
        "    for param in unet.parameters():\n",
        "        param.requires_grad_(False)\n",
        "\n",
        "    # for 10-12 VRAM\n",
        "    unet_lora_config = LoraConfig(\n",
        "      r=1,\n",
        "      lora_alpha=8,\n",
        "      init_lora_weights=\"gaussian\",\n",
        "      target_modules=[\"to_k\", \"to_q\", \"to_v\"],\n",
        "    )\n",
        "\n",
        "    # Move vae and text_encoder to device and cast to weight_dtype\n",
        "    # vae.to(\"cpu\")\n",
        "    # text_encoder.to(\"cpu\")\n",
        "    # text_encoder_2.to(\"cpu\")\n",
        "    # unet.to(\"cpu\")\n",
        "    # torch.cuda.empty_cache()\n",
        "    vae.to(accelerator.device, dtype=weight_dtype)\n",
        "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
        "    text_encoder_2.to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "    # Ensure unet is on device, but keep its parameters in float32 initially before adding adapter\n",
        "    unet.to(accelerator.device) # Keep UNet in float32 for now\n",
        "\n",
        "    # Add adapter and make sure the trainable params are in float32.\n",
        "    if not trained_model_path: # Only add adapter if no pre-trained LoRA was loaded\n",
        "        unet.add_adapter(unet_lora_config)\n",
        "    else: # If a trained model path is provided, it means LoRA was loaded, so ensure it's enabled\n",
        "        unet.enable_lora() # Explicitly enable LoRA for loaded weights\n",
        "\n",
        "    # This ensures LoRA layers, once added, are always in float32, which is important for GradScaler.\n",
        "    # The base UNet weights will be handled by accelerator during autocast.\n",
        "    # We explicitly cast trainable LoRA parameters to float32.\n",
        "    if accelerator.mixed_precision == \"fp16\": # Apply this only if we are using fp16\n",
        "        for name, param in unet.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                # keep param on same device but force float32 dtype\n",
        "                param.data = param.data.to(torch.float32)\n",
        "                # if a grad exists (unlikely at this point) cast it too\n",
        "                if param.grad is not None:\n",
        "                    param.grad.data = param.grad.data.to(torch.float32)\n",
        "\n",
        "    def unwrap_model(model):\n",
        "        model = accelerator.unwrap_model(model)\n",
        "        model = model._orig_mod if is_compiled_module(model) else model\n",
        "        return model\n",
        "\n",
        "    lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n",
        "\n",
        "    if gradient_checkpointing:\n",
        "        unet.enable_gradient_checkpointing()\n",
        "        logger.info(\"enabled unet.gradient_checkpointing()\")\n",
        "\n",
        "    learning_rate = (\n",
        "          1e-4 * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n",
        "      )\n",
        "\n",
        "    # Initialize the optimizer\n",
        "    if use_8bit_adam:\n",
        "        try:\n",
        "            import bitsandbytes as bnb\n",
        "        except ImportError:\n",
        "            raise ImportError(\n",
        "                \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n",
        "            )\n",
        "\n",
        "        optimizer_cls = bnb.optim.AdamW8bit\n",
        "    else:\n",
        "        optimizer_cls = torch.optim.AdamW\n",
        "\n",
        "    optimizer = optimizer_cls(\n",
        "        lora_layers,\n",
        "        lr=learning_rate,\n",
        "        betas=(adam_beta1, adam_beta2),\n",
        "        weight_decay=adam_weight_decay,\n",
        "        eps=adam_epsilon,\n",
        "    )\n",
        "\n",
        "    # Get the datasets: you can either provide your own training and evaluation files (see below)\n",
        "    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\n",
        "\n",
        "    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n",
        "    # download the dataset.\n",
        "    if dataset_type == \"huggingface\":\n",
        "        accelerator.print(f\"Loading the dataset: {dataset_name}\")\n",
        "        dataset = load_dataset(\n",
        "            dataset_name,\n",
        "            dataset_config_name,\n",
        "        )\n",
        "        column_names = dataset[\"train\"].column_names\n",
        "        image_column_name = image_column\n",
        "        caption_column_name = caption_column\n",
        "    else:\n",
        "        accelerator.print(f\"Loading the dataset from: {dataset_path}\")\n",
        "        dataset = load_dataset(\n",
        "            \"imagefolder\",\n",
        "            data_dir=dataset_path,\n",
        "        )\n",
        "        image_column_name = \"image\"\n",
        "        caption_column_name = \"text\"\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    def tokenize_captions(examples, is_train=True):\n",
        "        captions = []\n",
        "        for caption in examples[caption_column_name]:\n",
        "            if isinstance(caption, str):\n",
        "                captions.append(caption)\n",
        "            elif isinstance(caption, (list, np.ndarray)):\n",
        "                captions.append(random.choice(caption) if is_train else caption[0])\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f\"Caption column `{caption_column_name}` should contain either strings or lists of strings.\"\n",
        "                )\n",
        "        # For SDXL, we need two tokenizers\n",
        "        # First tokenizer for text_encoder\n",
        "        input_ids = tokenizer(\n",
        "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        ).input_ids\n",
        "        # Second tokenizer for text_encoder_2\n",
        "        input_ids_2 = tokenizer_2(\n",
        "            captions, max_length=tokenizer_2.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        ).input_ids\n",
        "        return input_ids, input_ids_2\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    train_transforms = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.CenterCrop(resolution) if center_crop else transforms.RandomCrop(resolution),\n",
        "            transforms.RandomHorizontalFlip() if random_flip else transforms.Lambda(lambda x: x),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    def preprocess_train(examples):\n",
        "        images = [image.convert(\"RGB\") for image in examples[image_column_name]]\n",
        "        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
        "        input_ids, input_ids_2 = tokenize_captions(examples) # Get both tokenized inputs\n",
        "        examples[\"input_ids\"] = input_ids\n",
        "        examples[\"input_ids_2\"] = input_ids_2\n",
        "\n",
        "        # For SDXL, we need to add original_size and crop_coords_top_left as well\n",
        "        # Assuming all images are resized to 'resolution' and then cropped, set original_size and crop_coords_top_left accordingly\n",
        "        examples[\"original_size\"] = [(resolution, resolution)] * len(images)\n",
        "        examples[\"crops_coords_top_left\"] = [(0, 0)] * len(images)\n",
        "\n",
        "        return examples\n",
        "\n",
        "    with accelerator.main_process_first():\n",
        "        if max_train_samples is not None and max_train_samples > 0:\n",
        "            dataset[\"train\"] = dataset[\"train\"].shuffle(seed=seed_size).select(range(max_train_samples))\n",
        "        train_dataset = dataset[\"train\"].with_transform(preprocess_train, output_all_columns=True)\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
        "        input_ids_2 = torch.stack([example[\"input_ids_2\"] for example in examples])\n",
        "        original_sizes = torch.tensor([example[\"original_size\"] for example in examples], dtype=torch.long)\n",
        "        crops_coords_top_left = torch.tensor([example[\"crops_coords_top_left\"] for example in examples], dtype=torch.long)\n",
        "\n",
        "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids, \"input_ids_2\": input_ids_2, \"original_sizes\": original_sizes, \"crops_coords_top_left\": crops_coords_top_left}\n",
        "\n",
        "    # DataLoaders creation:\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        batch_size=train_batch_size,\n",
        "        num_workers=dataloader_num_workers,\n",
        "    )\n",
        "\n",
        "    # Scheduler and math around the number of training steps.\n",
        "    num_warmup_steps_for_scheduler = lr_warmup_steps * accelerator.num_processes\n",
        "    if max_train_steps is None or max_train_steps <= 0:\n",
        "        len_train_dataloader_after_sharding = math.ceil(len(train_dataloader) / accelerator.num_processes)\n",
        "        num_update_steps_per_epoch = math.ceil(len_train_dataloader_after_sharding / gradient_accumulation_steps)\n",
        "        num_training_steps_for_scheduler = (\n",
        "            num_train_epochs * num_update_steps_per_epoch * accelerator.num_processes\n",
        "        )\n",
        "        max_train_steps_actual = num_train_epochs * num_update_steps_per_epoch\n",
        "    else:\n",
        "        num_training_steps_for_scheduler = max_train_steps * accelerator.num_processes\n",
        "        max_train_steps_actual = max_train_steps\n",
        "\n",
        "    # To avoid UnboundLocalError, explicitly access the global lr_scheduler string value\n",
        "    # before assigning a local lr_scheduler object.\n",
        "    scheduler_type_str = globals()['lr_scheduler']\n",
        "\n",
        "    lr_scheduler = get_scheduler(\n",
        "        scheduler_type_str, # Use the global lr_scheduler string directly\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=num_warmup_steps_for_scheduler,\n",
        "        num_training_steps=num_training_steps_for_scheduler,\n",
        "    )\n",
        "\n",
        "    # Prepare everything with our `accelerator`.\n",
        "    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        unet, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
        "\n",
        "    # Afterwards we recalculate our number of training epochs\n",
        "    num_train_epochs_actual = math.ceil(max_train_steps_actual / num_update_steps_per_epoch)\n",
        "\n",
        "    # Train!\n",
        "    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n",
        "\n",
        "    global_step = 0\n",
        "    first_epoch = 0\n",
        "    training_start_time = time.time()\n",
        "\n",
        "    # Initialize a local variable for resume_from_checkpoint to avoid UnboundLocalError\n",
        "    _resume_from_checkpoint = resume_from_checkpoint\n",
        "\n",
        "    # Potentially load in the weights and states from a previous save\n",
        "    if _resume_from_checkpoint and os.path.exists(output_dir):\n",
        "        # Get the most recent checkpoint\n",
        "        dirs = os.listdir(output_dir)\n",
        "        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
        "        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
        "        path = dirs[-1] if len(dirs) > 0 else None\n",
        "\n",
        "        if path is None:\n",
        "            accelerator.print(\n",
        "                \"Checkpoint does not exist. Starting a new training run.\"\n",
        "            )\n",
        "            _resume_from_checkpoint = None # Update local copy\n",
        "            initial_global_step = 0\n",
        "        else:\n",
        "            accelerator.print(f\"Resuming from checkpoint {path}\")\n",
        "            accelerator.load_state(os.path.join(output_dir, path))\n",
        "            global_step = int(path.split(\"-\")[1])\n",
        "\n",
        "            initial_global_step = global_step\n",
        "            first_epoch = global_step // num_update_steps_per_epoch\n",
        "\n",
        "    else:\n",
        "        accelerator.print(\"No previous checkpoint found. Starting a new training run.\")\n",
        "        initial_global_step = 0\n",
        "        path = None\n",
        "\n",
        "    # Weights & Biases integration\n",
        "    if wandb_project and os.environ.get('WANDB_API_KEY'):\n",
        "        # initialize_wandb(args, path) # initialize_wandb is not defined, replaced with accelerator.init_trackers\n",
        "        accelerator.init_trackers(project_name=wandb_project, config={\n",
        "            \"output_dir\": output_dir,\n",
        "            \"resolution\": resolution,\n",
        "            \"train_batch_size\": train_batch_size,\n",
        "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "            \"lr\": learning_rate,\n",
        "            \"num_train_epochs\": num_train_epochs_actual,\n",
        "            \"max_train_steps\": max_train_steps_actual,\n",
        "            \"lr_scheduler\": lr_scheduler,\n",
        "            \"seed\": seed_size,\n",
        "            \"mixed_precision\": accelerator.mixed_precision,\n",
        "            \"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"image_column\": image_column,\n",
        "            \"caption_column\": caption_column,\n",
        "        }, init_kwargs={\"resume\": \"allow\" if _resume_from_checkpoint else None})\n",
        "    else:\n",
        "        # If wandb not enabled, still need to initialize accelerator trackers for TensorBoard\n",
        "        accelerator.init_trackers(project_name=\"tensorboard_logs\", config={\n",
        "            \"output_dir\": output_dir,\n",
        "            \"resolution\": resolution,\n",
        "            \"train_batch_size\": train_batch_size,\n",
        "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "            \"lr\": learning_rate,\n",
        "            \"num_train_epochs\": num_train_epochs_actual,\n",
        "            \"max_train_steps\": max_train_steps_actual,\n",
        "            \"lr_scheduler\": lr_scheduler,\n",
        "            \"seed\": seed_size,\n",
        "            \"mixed_precision\": accelerator.mixed_precision,\n",
        "            \"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"image_column\": image_column,\n",
        "            \"caption_column\": caption_column,\n",
        "        }) # Passing a project_name for tensorboard only, it won't push to wandb.\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        range(0, max_train_steps_actual),\n",
        "        initial=initial_global_step,\n",
        "        desc=\"Steps\",\n",
        "        # Only show the progress bar once on each machine.\n",
        "        disable=not accelerator.is_local_main_process,\n",
        "    )\n",
        "\n",
        "    # Initialize TensorBoard writer\n",
        "    tensorboard_writer = SummaryWriter(log_dir=logging_dir)\n",
        "\n",
        "    # Initialize the Trainer\n",
        "    trainer = StableDiffusionTrainer(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_train_epochs_actual,\n",
        "        max_train_steps=max_train_steps_actual,\n",
        "        logging_steps=logging_steps,\n",
        "        train_dataloader=train_dataloader, vae=vae, unet=unet, text_encoder=text_encoder, text_encoder_2=text_encoder_2, noise_scheduler=noise_scheduler, optimizer=optimizer, lr_scheduler=lr_scheduler, accelerator=accelerator, weight_dtype=weight_dtype, tensorboard_writer=tensorboard_writer, logger=logger, progress_bar=progress_bar, first_epoch=first_epoch, initial_global_step=initial_global_step, unwrap_model=unwrap_model,\n",
        "    )\n",
        "    # Start the training\n",
        "    trainer.train()\n",
        "    training_end_time = time.time()\n",
        "\n",
        "    # Save the final model as checkpoint-final\n",
        "    save_path = os.path.join(output_dir, \"final\")\n",
        "    accelerator.save_state(save_path)\n",
        "    unwrapped_unet = unwrap_model(unet)\n",
        "    unet_lora_state_dict = convert_state_dict_to_diffusers(\n",
        "        get_peft_model_state_dict(unwrapped_unet)\n",
        "    )\n",
        "    StableDiffusionPipeline.save_lora_weights(\n",
        "        save_directory=save_path,\n",
        "        unet_lora_layers=unet_lora_state_dict,\n",
        "        safe_serialization=True,\n",
        "    )\n",
        "    metrics = trainer.get_final_metrics()\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "    if accelerator.is_main_process:\n",
        "        # run inference\n",
        "        validation_start_time = time.time()\n",
        "        if validation_prompt and num_validation_images > 0:\n",
        "            # Make sure vae.dtype is consistent with the unet.dtype\n",
        "            if accelerator.mixed_precision == \"fp16\":\n",
        "                vae.to(weight_dtype)\n",
        "            # Load previous pipeline\n",
        "            pipeline = DiffusionPipeline.from_pretrained(\n",
        "                pretrained_model_name_or_path,\n",
        "                revision=revision,\n",
        "                variant=variant,\n",
        "                torch_dtype=weight_dtype,\n",
        "            )\n",
        "            # load attention processors\n",
        "            pipeline.load_lora_weights(save_path)\n",
        "\n",
        "            # log_validation function is not defined in the provided context, commenting out.\n",
        "            # images = log_validation(pipeline, SimpleNamespace(num_validation_images=num_validation_images,\n",
        "            #                                                 resolution=resolution, output_dir=output_dir,\n",
        "            #                                                 validation_prompt=validation_prompt),\n",
        "            #                         accelerator, num_train_epochs_actual, tensorboard_writer, is_final_validation=True)\n",
        "            # For demonstration, creating dummy images if log_validation is skipped.\n",
        "            images = [Image.new(\"RGB\", (resolution, resolution), color = 'red')] * num_validation_images\n",
        "\n",
        "            # calculate_clip_score function is not defined in the provided context, commenting out.\n",
        "            # clip_score = calculate_clip_score(images, validation_prompt, clip_model, preprocess, accelerator.device)\n",
        "            # logger.info(f\"CLIP score: {clip_score}\")\n",
        "            # metrics[\"clip_score\"] = clip_score\n",
        "            logger.info(\"Skipped CLIP score calculation because `calculate_clip_score` is not defined.\")\n",
        "            del pipeline\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        validation_end_time = time.time()\n",
        "\n",
        "    # Close the Writers\n",
        "    tensorboard_writer.close()\n",
        "\n",
        "    accelerator.end_training()\n",
        "    # push_model(output_dir, metrics) # push_model function is not defined, commenting out."
      ],
      "metadata": {
        "id": "HUmB2g5GQtFV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Aw0j6wgfSIGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0e9350cb4e9a4439b0cc9fc8316b2a62",
            "90c41d5116f147ee95ce7421b7039a7e",
            "8b7680a8d0c24accaa7bc56674bf68f4",
            "5d9bfc736507464fa223cdb1576d2e1e",
            "7748ef307fc140edb8cd794327119219",
            "1df0054e9fbc4e96bd807ca1e9bd2b5d",
            "d8a2524d3f3d499f9aaadd882ee67b1d",
            "a8b1027297304b738deb83c8d07d13e1",
            "bb3c99057f9c4a64a7056d11adafbcc4",
            "071901cf9ab54eefbbde302dbfadb2df",
            "e06b622e5a814b0eb81f105818f18283"
          ]
        },
        "outputId": "a062f8d5-159b-44d3-9e47-6ee96cf13b19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the tokenizers, text endcoders, schedulers and model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the Stable diffusion Model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "{'variance_type', 'thresholding', 'clip_sample_range', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
            "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
            "{'mid_block_add_attention', 'latents_std', 'use_post_quant_conv', 'latents_mean', 'shift_factor', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "{'attention_type', 'dropout', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "WARNING:__main__:xformers not available: Refer to https://github.com/facebookresearch/xformers for more information on how to install xformers\n",
            "WARNING:__main__:attention slicing failed: 'UNet2DConditionModel' object has no attribute 'enable_attention_slicing'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the CLIP Model for validation:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADING PRE-TRAINED MODEL FROM \n",
            "No source model path provided, LoRA will not be loaded initially.\n",
            "Loading the dataset: lambdalabs/naruto-blip-captions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous checkpoint found. Starting a new training run.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manonymous7770777\u001b[0m (\u001b[33manonymous7770777-e2e\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251125_094128-9hd3ny9m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/anonymous7770777-e2e/tensorboard_logs/runs/9hd3ny9m' target=\"_blank\">ethereal-thunder-15</a></strong> to <a href='https://wandb.ai/anonymous7770777-e2e/tensorboard_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/anonymous7770777-e2e/tensorboard_logs' target=\"_blank\">https://wandb.ai/anonymous7770777-e2e/tensorboard_logs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/anonymous7770777-e2e/tensorboard_logs/runs/9hd3ny9m' target=\"_blank\">https://wandb.ai/anonymous7770777-e2e/tensorboard_logs/runs/9hd3ny9m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Steps:   0%|          | 0/612 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e9350cb4e9a4439b0cc9fc8316b2a62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 499691 has 14.72 GiB memory in use. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 217.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3832242952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-265890866.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    382\u001b[0m     )\n\u001b[1;32m    383\u001b[0m     \u001b[0;31m# Start the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0mtraining_end_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1352089874.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0;31m# UNet forward in autocast (mixed precision)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                         model_pred = self.unet(\n\u001b[0m\u001b[1;32m    104\u001b[0m                             \u001b[0mnoisy_model_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                             \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     decorate_autocast.__script_unsupported = (  # type: ignore[attr-defined]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/unets/unet_2d_condition.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid_block\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"has_cross_attention\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1245\u001b[0;31m                 sample = self.mid_block(\n\u001b[0m\u001b[1;32m   1246\u001b[0m                     \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m                     \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/unets/unet_2d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m                 hidden_states = attn(\n\u001b[0m\u001b[1;32m    881\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/transformers/transformer_2d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 )\n\u001b[1;32m    426\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                 hidden_states = block(\n\u001b[0m\u001b[1;32m    428\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0mff_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_chunked_feed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunk_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m             \u001b[0mff_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ada_norm_zero\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1729\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scale\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprecation_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1731\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/activations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mdeprecation_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scale\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprecation_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_torch_npu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# using torch_npu.npu_geglu can run faster and save memory on NPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 499691 has 14.72 GiB memory in use. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 217.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2f_KWzaEcXec"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}